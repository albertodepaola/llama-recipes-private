{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e39434-e57a-406f-8d26-adff51c10fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 9906, 4435, 0, 128001]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "from llama_recipes.inference.llama.generation import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9fabd45-e2f2-42bc-a7ab-703b367d9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(user_input: str) -> str:\n",
    "    # TODO flesh out this function\n",
    "    return f\"<header_start> user <header_end>\\n\\n{user_input}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b7a700f-37b3-47fa-9bd5-9a588c7c3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables used by the Llama classes\n",
    "import os\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29501\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d31f7a-466b-4194-897a-54eeedb1c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_llm_eval(prompts: List[str], ckpt_dir, logprobs: bool = False):\n",
    "    # defaults\n",
    "    temperature = 0.6\n",
    "    top_p = 1\n",
    "    max_seq_len = 2048\n",
    "    max_gen_len = 1024\n",
    "    max_batch_size = 1\n",
    "\n",
    "    generator = Llama.build(\n",
    "            ckpt_dir=ckpt_dir,\n",
    "        # TODO this is not necessary with the new tokenizer\n",
    "            tokenizer_path=ckpt_dir + \"/tokenizer.model\",\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "        )\n",
    "\n",
    "\n",
    "    results: List[str] = []\n",
    "    total_length = len(prompts)\n",
    "    progress_bar = tqdm(colour=\"blue\", desc=f\"Prompts\", total=total_length, dynamic_ncols=True)\n",
    "    for prompt in prompts:\n",
    "        # TODO format the prompts according to the new llama format\n",
    "        # formatted_prompt = build_prompt(\n",
    "        #         prompt[\"agent_type\"], \n",
    "        #         LLAMA_GUARD_CATEGORY, \n",
    "        #         create_conversation(prompt[\"prompt\"]))\n",
    "        formatted_prompt = format_prompt(prompt)\n",
    "\n",
    "        result = generator.text_completion(\n",
    "            [formatted_prompt],\n",
    "            temperature,\n",
    "            top_p,\n",
    "            max_gen_len,\n",
    "            logprobs\n",
    "        )\n",
    "        # getting the first value only, as only a single prompt was sent to the function\n",
    "        generation_result = result[0][\"generation\"]\n",
    "        # prompt[\"result\"] = generation_result\n",
    "        # if logprobs:\n",
    "        #     prompt[\"logprobs\"] = result[0][\"logprobs\"]\n",
    "\n",
    "        results.append(generation_result)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f828fe0-2897-4e18-b392-2f88646b0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(ckpt_dir: str):\n",
    "   \n",
    "    prompts: List[str] = [\n",
    "        \"Hi, how are you doing?\", \n",
    "        \"What can you help me with?\", \n",
    "        \"What is the capital of France?\", \n",
    "        \"Should civilians own guns?\"\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # results = llm_eval(prompts, load_in_8bit = False, load_in_4bit = True)\n",
    "    # The model is in fsdp, does not have a tokenizer\n",
    "    results = pytorch_llm_eval(prompts, ckpt_dir=ckpt_dir)\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(prompt)\n",
    "        print(f\"> {results[i]}\")\n",
    "        print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96fdf7-4eaa-4a82-a731-4b53133867ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    }
   ],
   "source": [
    "main(\"/home/ubuntu/projects/llama/models/l3p-7b-base-model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f3eb8-ebfa-4962-ba6b-76cf9d9f7803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
